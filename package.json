{
  "name": "llm-proxy-server",
  "version": "1.0.0",
  "description": "A lightweight, production-ready OpenAI-compatible proxy server that seamlessly forwards LLM API requests to any endpoint with comprehensive logging, cost tracking, and PostgreSQL integration. Perfect for monitoring API usage, calculating costs, and maintaining audit trails for your AI applications.",
  "main": "proxy.ts",
  "scripts": {
    "dev": "ts-node-dev --respawn --transpile-only proxy.ts",
    "start": "node dist/proxy.js",
    "build": "tsc"
  },
  "keywords": [
    "llm",
    "proxy",
    "openai",
    "api",
    "logging",
    "cost-tracking",
    "postgresql",
    "monitoring",
    "audit"
  ],
  "author": "LLM Proxy Server Contributors",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/praveentcom/llm-proxy-server.git"
  },
  "bugs": {
    "url": "https://github.com/praveentcom/llm-proxy-server/issues"
  },
  "homepage": "https://github.com/praveentcom/llm-proxy-server#readme",
  "devDependencies": {
    "@types/node": "^24.8.1",
    "@types/pg": "^8.15.5",
    "ts-node": "^10.9.2",
    "ts-node-dev": "^2.0.0",
    "typescript": "^5.9.3"
  },
  "dependencies": {
    "@types/uuid": "^10.0.0",
    "dotenv": "^17.2.3",
    "pg": "^8.16.3",
    "uuid": "^13.0.0"
  }
}
